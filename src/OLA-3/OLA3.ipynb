{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../OLA-2/data/raw/heart_2020_cleaned_transformed.csv\")\n",
    "\n",
    "# Assuming df is your DataFrame, X is your feature matrix and y is your target variable\n",
    "correlation = df.corr()\n",
    "\n",
    "# Get correlation between each feature and target variable\n",
    "corr_target = abs(correlation[\"HeartDisease\"])\n",
    "\n",
    "sorted_corr = corr_target.sort_values(ascending=False)\n",
    "print(sorted_corr[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most relevant features aer: GenHealth, AgeCategory, DiffWalking, Stroke, Diabetic, PhysicalHealth\n",
    "features = df[\n",
    "    [\"GenHealth\", \"AgeCategory\", \"DiffWalking\", \"Stroke\", \"Diabetic\", \"PhysicalHealth\"]\n",
    "]\n",
    "\n",
    "# Handle missing values if any. For instance, dropping rows with missing values\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Elbow Method\n",
    "distortions = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_scaled)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(2, 10), distortions, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "# k = 6  # Choose the number of clusters based on the elbow diagram\n",
    "# kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# kmeans.fit(features_scaled)\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# score = silhouette_score(features_scaled, labels)\n",
    "# print(f\"Silhouette Score: {score}\")\n",
    "\n",
    "# # Silhouette Plot\n",
    "# from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# silhouette_vals = silhouette_samples(features_scaled, labels)\n",
    "\n",
    "# y_ax_lower, y_ax_upper = 0, 0\n",
    "# yticks = []\n",
    "# for i, c in enumerate(sorted(set(labels))):\n",
    "#     c_silhouette_vals = silhouette_vals[labels == c]\n",
    "#     c_silhouette_vals.sort()\n",
    "#     y_ax_upper += len(c_silhouette_vals)\n",
    "#     color = plt.cm.nipy_spectral(float(i) / len(set(labels)))\n",
    "#     plt.barh(\n",
    "#         range(y_ax_lower, y_ax_upper),\n",
    "#         c_silhouette_vals,\n",
    "#         height=1.0,\n",
    "#         edgecolor=\"none\",\n",
    "#         color=color,\n",
    "#     )\n",
    "#     yticks.append((y_ax_lower + y_ax_upper) / 2.0)\n",
    "#     y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "# silhouette_avg = np.mean(silhouette_vals)\n",
    "# plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# plt.yticks(yticks, [str(c) for c in sorted(set(labels))])\n",
    "# plt.ylabel(\"Cluster\")\n",
    "# plt.xlabel(\"Silhouette score\")\n",
    "# plt.show()\n",
    "\n",
    "# num_clusters = 6\n",
    "\n",
    "# # Apply KMeans\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# kmeans.fit(features_scaled)\n",
    "\n",
    "# # Get the cluster labels for each data point\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# # Add the labels to the original DataFrame\n",
    "# df['cluster'] = labels\n",
    "\n",
    "# # Print the first few rows of the DataFrame to see the cluster labels\n",
    "# print(df.head())\n",
    "\n",
    "# # Visualizing the clusters is a bit tricky when you have more than two features.\n",
    "# # One common approach is to use a pairplot, which shows scatter plots for each pair of features.\n",
    "# # However, this requires all features to be numerical. If you have categorical features,\n",
    "# # you'll need to handle them appropriately (e.g., by using one-hot encoding).\n",
    "# # Here's an example of how you might create a pairplot using seaborn:\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.pairplot(df[['GenHealth', 'AgeCategory', 'DiffWalking', 'Stroke', 'Diabetic', 'cluster']], hue='cluster')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose the number of clusters\n",
    "num_clusters = 6\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(features_scaled)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the labels to the original DataFrame\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# Print the first few rows of the DataFrame to see the cluster labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### too computational intensive\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# # Apply DBSCAN\n",
    "# # eps is the maximum distance between two samples for them to be considered as in the same neighborhood\n",
    "# # min_samples is the minimum number of samples in a neighborhood for a point to be considered as a core point\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "# dbscan.fit(features_scaled)\n",
    "\n",
    "# # Get the cluster labels for each data point\n",
    "# labels = dbscan.labels_\n",
    "\n",
    "# # Add the labels to the original DataFrame\n",
    "# df[\"cluster\"] = labels\n",
    "\n",
    "# # Print the first few rows of the DataFrame to see the cluster labels\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining with logistic regression ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average value of each feature for each cluster\n",
    "for cluster in set(labels):\n",
    "    cluster_data = features[labels == cluster]\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(cluster_data.mean())\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Add cluster labels as a new feature\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"HeartDisease\", axis=1), df[\"HeartDisease\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pairplot of the features, colored by the cluster labels\n",
    "sns.pairplot(df[['GenHealth', 'AgeCategory', 'DiffWalking', 'Stroke', 'Diabetic', 'cluster']], hue='cluster')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hafla mistral:\n",
    "    Discuss the limitations and assumptions of the clustering algorithms: Here's an example of how you can discuss the limitations and assumptions of KMeans:\n",
    "\n",
    "KMeans is a widely used clustering algorithm that aims to partition the data into K distinct non-overlapping clusters such that each data point belongs to the cluster with the nearest mean. However, KMeans has several limitations and assumptions:\n",
    "\n",
    "    KMeans assumes that the clusters are spherical and have similar densities, which might not be the case in real-world datasets.\n",
    "    KMeans is sensitive to the initialization of the centroids, which can lead to different clustering results for different runs of the algorithm.\n",
    "    KMeans is not suitable for categorical data, as it relies on the computation of the mean of the data points.\n",
    "    KMeans requires the number of clusters to be specified in advance, which can be challenging to determine in some cases.\n",
    "\n",
    "    Discuss the implications of the findings: Here's an example of how you can discuss the implications of the findings:\n",
    "\n",
    "The clustering results suggest that there are distinct groups of patients with different risk profiles based on their characteristics. For example, Cluster 0 seems to be characterized by younger patients with good general health and no history of stroke or diabetes, while Cluster 1 seems to be characterized by older patients with poor general health and a history of stroke or diabetes. These findings could have important implications for the diagnosis and treatment of heart disease. For example, patients in Cluster 1 might benefit from more aggressive interventions to manage their risk factors, while patients in Cluster 0 might require less intensive interventions. However, further research is needed to validate these findings and to explore the potential clinical applications of the identified clusters.\n",
    "\n",
    "    Proofread the notebook: Make sure to proofread the notebook to catch any spelling or grammar errors, and to ensure that the code runs correctly and the explanations are clear and concise. You might want to use a tool like Grammarly or Hemingway Editor to help you with proofreading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Cost: The Silhouette Score is computationally expensive, especially for large datasets. Your project's dataset is sizable, and calculating the Silhouette Score for various values of k can be time-consuming and resource-intensive.\n",
    "\n",
    "Complexity for Multi-Dimensional Data: The Silhouette Score might not be as effective for datasets with many features, as it essentially measures how close each point in one cluster is to points in the neighboring clusters. This can be less interpretable when dealing with multi-dimensional data, where the notion of distance becomes less intuitive.\n",
    "\n",
    "Potential Misleading Results in Certain Cases: In scenarios where clusters have varying densities or non-globular shapes, the Silhouette Score might not provide a clear indication of the best number of clusters. For heart disease data, which could have complex and overlapping patterns, the score might not reflect the true clustering tendency.\n",
    "\n",
    "Sensitivity to Noise and Outliers: DBSCAN is a clustering algorithm designed to handle noise and outliers effectively by identifying them as separate from core clusters. The Silhouette Score, however, does not distinguish between noise/outliers and legitimate data points, potentially leading to misleading evaluations of cluster quality.\n",
    "\n",
    "Elbow Method as a Simpler Alternative: The Elbow Method is a simpler and more computationally efficient approach. Although it might be less precise than the Silhouette Score, it can provide a good-enough estimate for the optimal number of clusters, which can be sufficient for many practical applications, including preliminary explorations in healthcare data.\n",
    "\n",
    "Project Focus on DBSCAN: If your project's primary focus is on exploring DBSCAN as a clustering technique, which doesn't require pre-specifying the number of clusters, then the Silhouette Score loses its relevance, as it is generally more useful for algorithms like KMeans, where determining the number of clusters is crucial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
